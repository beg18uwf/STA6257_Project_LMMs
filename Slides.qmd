---
title: "Linear Mixed-Effects Models"
author: "Nick Freeland, Bernice Green, Gary Marmon"
format:
  revealjs: 
    theme: league
    preview-links: auto
---

# Introduction

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
#| warning: false
#| message: false
#| include: false

# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(nflverse)
library(nflplotR)
library(kableExtra)
library(GLMsData)
library(ggfortify)
library(gridExtra)
library(lme4)
library(lmerTest)

# loading play by play data from the 2021 NFL season
pbp <- nflreadr::load_pbp(2021)

# team summaries 
team_sum1 <- data.frame(pbp$game_id, pbp$home_coach, pbp$away_coach, pbp$posteam, pbp$posteam_type, pbp$pass, pbp$rush, pbp$epa, pbp$down, pbp$week, pbp$season_type, pbp$yards_gained, pbp$shotgun, pbp$no_huddle, pbp$yards_after_catch)

team_sum2 <- team_sum1 %>%
  filter(pbp.rush == 1 | pbp.pass == 1, !is.na(pbp.down)) %>%
  group_by (pbp.posteam, pbp.posteam_type, pbp.game_id) %>%
  mutate(coach = ifelse(pbp.posteam_type == 'home',pbp.home_coach,pbp.away_coach)) %>%
  mutate(opp_coach = ifelse(pbp.posteam_type == 'away',pbp.home_coach,pbp.away_coach)) %>%
  mutate(home_adv = ifelse(pbp.posteam_type == 'home', 0, 1))%>%
  summarize(week = first(pbp.week),
            season_type = first(ifelse(pbp.season_type == 'REG', 0, 1)),
            home_adv = first(home_adv),
            coach = first(coach),
            opp_coach = first(opp_coach),
            plays = n(),
            pass_plays = sum(pbp.pass),
            pass_pct = pass_plays / plays,
            yards_gained = sum(pbp.yards_gained),
            shotgun_snaps = sum(pbp.shotgun),
            no_huddle_snap = sum(pbp.no_huddle),
            epa_per_play = round(mean(pbp.epa), digits = 2))

```

## Simple Linear Regressions

-   assumes independence of observations

-   slopes and intercepts measure average trends

```{r Linear Regression, echo=FALSE, message=FALSE, warning=FALSE}
x <- c(9,9,7,8,7,8,4,5,5,4,6,2,1,3,3,2,1,2,1,0,2,8,10,9,7.5,6,10)
y <- c(0,1,0,2,2,1,3,4,5,4,6,5,5,6,8,7,7,8,9,9,9,5,3,4,0,0,4)
Labels <- c('A','A','A','A','A','A','B','B','B','B','B','B','B','B','C','C','C','C','C','C','C','A','A','A','A','A','A')


data <- data.frame(x,y,Labels)

slr <- ggplot(data, aes(x, y)) + 
    geom_point(size=4) + # change size and colour
   scale_y_continuous(limits = c(0, 10), breaks = c(0, 2, 4, 6, 8, 10)) + # y axis limits/range (0, 100), break points
    scale_x_continuous(limits = c(0, 10), breaks = c(0, 2, 4, 6, 8, 10)) + # x axis limits/range 
    geom_smooth(method = 'lm', se = F) # fit linear regression line

slr
```

# But...

What if the observations are not independent?

What if trends vary between clusters?

```{r message=FALSE, warning=FALSE}
slr.col <- ggplot(data, aes(x, y, col = Labels)) + 
    geom_point(size=4) + # change size and colour
    scale_y_continuous(limits = c(0, 10), breaks = c(0, 2, 4, 6, 8, 10)) + # y axis limits/range (0, 100), break points
    scale_x_continuous(limits = c(0, 10), breaks = c(0, 2, 4, 6, 8, 10)) + # x axis limits/range 
    geom_smooth(method = 'lm', se = F, aes(group = 1)) # fit linear regression line

slr.col
```

## Linear Mixed-Effects Models (LMMs)

-   LMMs can be used to model correlated data
    -   Cross sectional data: individuals nested in a geographical or social context - *ex: Student test data compared across Geometry classes*

    -   Longitudinal data: repeated measures of individuals - *ex: Student test data over time*

Main application for mixed-effect models is in psychology due to the nature of their data and repeated observations across trial participants

```{r message=FALSE, warning=FALSE}
lmm <- ggplot(data, aes(x, y, col = Labels)) + 
    geom_point(size=4) + # change size and colour
    scale_y_continuous(limits = c(0, 10), breaks = c(0, 2, 4, 6, 8, 10)) + # y axis limits/range (0, 100), break points
    scale_x_continuous(limits = c(0, 10), breaks = c(0, 2, 4, 6, 8, 10)) + # x axis limits/range 
    geom_smooth(method = 'lm', se = F)  # fit linear regression line

lmm
```

## Fixed vs. Random Effects

*Fixed Effects* 

  -   show average trends for the population

  -   assume constant relationship between response and exploratory features

  -   infer/predict about levels included in the training data

## Fixed vs. Random Effects (cont.)

*Random Effects*

  -   show how trends vary across groups

  -   assumes constant relationship between response and exploratory features but relationship may vary between groups

  -   infer/predict about all levels in the population

  -   could be random intercepts, random slopes, or both

## Fixed and Random Effects {.smaller}

-   mixed-effects models simultaneously model fixed and random effects

-   fixed effect parameters

    -   small number of clusters, large number of observations

-   random effect parameters

    -   large number of clusters, small number of clusters per observation

. . .

**It's important to include the random effects in the model since fixed effects only give a partial picture of hierarchical data.**

## Linear Model vs Linear Mixed-Effects Model

```{r message=FALSE, warning=FALSE}
slr.lmm <- ggplot(data, aes(x, y, col = Labels)) + 
    geom_point(size=4) + # change size and colour
    scale_y_continuous(limits = c(0, 10), breaks = c(0, 2, 4, 6, 8, 10)) + # y axis limits/range (0, 100), break points
    scale_x_continuous(limits = c(0, 10), breaks = c(0, 2, 4, 6, 8, 10)) + # x axis limits/range 
    geom_smooth(method = 'lm', se = F) + # fit linear regression line
    geom_smooth(method = 'lm', se = F, aes(group = 1)) # fit linear regression line

slr.lmm
```

# Methods

## Overview {.smaller}

Until more recently the only way to handle the type of data mixed-effects model does was through repeated measures ANOVAs. Mixed-effects models are much more versatile in handling variability within and across groups and can handle missing data, providing much better results than the ANOVAs. (Brown 2021)

- Mixed-effects models more versatile with missing data and generally better predictions

## Applications {.smaller}

The main application for mixed-effect models is in psychology due to the nature of their data and repeated observations across trial participants. However, the applications can extend into almost any field where the variability across a group/person is desired in the analysis. One such example is the use of mixed-effects models on published health data sets to explore the link between smoking and depression in which it was found “Smoking status is robustly associated with depression (or depressive symptomatology) at 1 ½ to 2 times the risk of nonsmoking across a variety of study designs, depression measurements, and participant populations” (Luger, Suls, and Vander Weg 2014).

- Mixed effects models: regression with variability across groups

## Assumptions {.smaller}

The complex nature of mixed-effects models call into question the robustness of these models and brings more focus to the model assumptions. “Mixed-effects models involve complex fitting procedures and make several assumptions, in particular about the distribution of residual and random effects. Violations of these assumptions are common in real datasets, yet it is not always clear how much these violations matter to accurate and unbiased estimation.” (Schielzeth et al. 2020). The study found mixed-effects models to be very robust to violations of these assumptions, finding the estimates were unbiased and missing random effect predictors had little effect on the fixed effect estimates but had some effects on the estimates of random effects.

- Mixed-effects models offer good prediction even if assumptions on distribution are violated

## Formula {.smaller}

- The linear mixed effects model is represented as:

$$
y_i = X_iβ + Z_ib_i + e_i
$$

- $X_i$ is the design matrix of the fixed effects

- $\beta$ are the coefficients for the fixed effects

- $e_i$ is the error term

- Note the second design matrix $Z_i$ and the subject specific random effects $b_i$

  - These $b_i$ allow us to model the subject specific means and enable us to capture the marginal dependence among the observations
  
# Data 

## Data 

::: {.panel-tabset}

### Data Set
- We used the nflfastR package which tracks NFL play by play data (372 variables) for every game and features models for Expected Points, Win Probability, and Completion Probability
- The following variables were considered for our model

### -1-
  - pbp.posteam: the team with possession of the ball (offense)
  - pbp.posteam_type: specifies if the possessing team is home or away
  - pbp.game_id: the specific game id from the NFL

### -2-
  - week: week number in the season that the game was played
  - season_type: flag that specifies if it is a regular (0) or post (1) season game
  - home_adv: flag that specifies home (0) or away(1)

### -3-
  - coach: the coach of the team with possession (offensive plays)
  - opp_coach: the coach of the opposing team (defensive plays)
  - EPA_per_play: expected points added per play (nflfastr model)
  
### -4-
  - plays: total number of rush and pass plays given the team and game
  - pass_plays: number of pass plays ran given the team and game
  - pass_pct: the percentage of pass plays in the game calculated by pass_plays/plays
  
### -5-
  - yards_gained: yards gained by an offense
  - shotgun_snaps: number of snaps a team lined up in a shotgun formation
  - ho_huddle_snaps: number of snaps a team used a no huddle offense
 
:::

## Data Summary
![](Slides_files\figure-revealjs\Data Summary.png)

## Expected Points Added (EPA)  {.smaller}

::: {.panel-tabset}

### EPA
Expected points added are predicted for every play. 

We use this on a per play basis as our response variable

### Offense vs. Defense EPA
```{r message=FALSE, warning=FALSE}

offense <- pbp %>%
  dplyr::group_by(team = posteam) %>%
  dplyr::summarise(off_epa = mean(epa, na.rm = TRUE))
defense <- pbp %>%
  dplyr::group_by(team = defteam) %>%
  dplyr::summarise(def_epa = mean(epa, na.rm = TRUE))
offense %>%
  dplyr::inner_join(defense, by = "team") %>%
  ggplot2::ggplot(aes(x = off_epa, y = def_epa)) +
  ggplot2::geom_abline(slope = -1.5, intercept = c(.4, .3, .2, .1, 0, -.1, -.2, -.3), alpha = .2) +
  nflplotR::geom_mean_lines(aes(h_var = off_epa, v_var = def_epa)) +
  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.07, alpha = 0.7) +
  ggplot2::labs(
    x = "Offense EPA/play",
    y = "Defense EPA/play",
    title = "2021 NFL Offensive and Defensive EPA per Play"
  ) +
  ggplot2::theme_bw() +
  ggplot2::theme(
    plot.title = ggplot2::element_text(size = 12, hjust = 0.5, face = "bold")
  ) +
  ggplot2::scale_y_reverse()
```


### Variables vs. EPA 

```{r message=FALSE, warning=FALSE}
# plot each variable against epa to see if any patterns exist 

p1 <- ggplot(team_sum2, aes(x=pass_pct, y=epa_per_play)) +
  geom_point() + geom_smooth(method = "loess")

p2 <- ggplot(team_sum2,aes(x=week, y=epa_per_play)) + 
  geom_point() + geom_smooth(method = "loess")

p3 <- ggplot(team_sum2, aes(x=plays, y=epa_per_play)) + 
  geom_point() + geom_smooth(method = "loess")

p4 <- team_sum2 |>
  ggplot(aes(y=epa_per_play, x=yards_gained)) + 
  geom_point() + geom_smooth(method = "loess")

p5 <- team_sum2 |>
  ggplot(aes(y=epa_per_play, x=shotgun_snaps)) + 
 geom_point() + geom_smooth(method = "loess")

p6 <- team_sum2 |>
  ggplot(aes(y=epa_per_play, x=no_huddle_snap)) + 
  geom_point() + geom_smooth(method = "loess")

team_sum2$season_type <- as.factor(team_sum2$season_type)
team_sum2$home_adv <- as.factor(team_sum2$home_adv)

p7 <- ggplot(team_sum2,aes(x=season_type, y=epa_per_play)) + geom_boxplot()

p8 <- ggplot(team_sum2, aes(home_adv, epa_per_play)) + geom_boxplot()

# arrange visualizations in a grid
grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8)
```

:::

## EPA per Play by Coach {.smaller}

::: {.panel-tabset}

### Comparisons
```{r message=FALSE, warning=FALSE}

ggplot(team_sum2, aes(epa_per_play)) +
  geom_boxplot() +
  facet_wrap(~ coach) +
  theme_minimal()
  
```

### Observations
- note the variation between coaches (our subjects)
- we must account for this in our model (we do this by adding a random effect)

**Our basic model: $$epa = plays + (1+plays|coach)$$**

::: 

# Analysis

## LME4 Package {.smaller}

- Extension of the lmer function, which has become the predominant tool in the R language for fitting linear mixed-effect models. (Bates, Maechler & Bolker, 2012)
- As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms
  -  resp ~ FEexpr + (REexpr1|factor1) + (REexpr2|factor2) + ...
  - (REexpr1 | factor1) and (REexpr2 | factor2), determine both the random-effects model matrix
-Each random-effects term is of the form (expr|factor). The expression expr is evaluated as a linear model formula, producing a model matrix following the same rules used in standard R modelling functions (e.g., lm or glm). The expression factor is evaluated as an R factor.


## Starting with a simple linear model {.smaller}

```{r}
#| include: true
#| echo: true
#| warning: false
#| message: false
epa.lm = lm(epa_per_play ~ pass_pct + plays + yards_gained + shotgun_snaps + no_huddle_snap, 
            data=team_sum2)
summary(epa.lm)
```

- After accounting for the other variables, the number of shotgun snaps an offense runs does not appear to significantly effect epa per play
- Current model explains approximately 63% of the variance in epa per play. 

## Diagnostics

```{r}
#| include: true
#| echo: true
#| warning: false
#| message: false
autoplot(epa.lm)
```

- a few outliers are present, but the overall fit looks acceptable.
- we will move forward with a linear approach

## Linear Mixed Effects Model {.smaller}

- We want to account for a teams coach and if a team has home field advantage as random effects. 

```{r}
#| include: true
#| echo: true
#| warning: false
#| message: false

# LMM - random intercepts
epa.lmer1 = lmer(epa_per_play ~ pass_pct + plays + yards_gained + no_huddle_snap + 
                (1|coach) + (1|home_adv), data=team_sum2)

epa.lmer1
```

## No p-value?

- Unfortunately, p-values	for	mixed	models aren’t as straightforward as	they are for the linear model.
- While the null distributions are asymptotically normal, these distributions are not t distributed for finite size samples — nor are the corresponding null distributions of differences in scaled deviances F distributed

## Statistical Significance (Liklihood Ratio Test) {.smaller}

```{r}
#| include: true
#| echo: true
#| warning: false
#| message: false

# Testing for significance between models with and without home field advantage
epa.lmer2.null = lmer(epa_per_play ~ pass_pct + plays + yards_gained + shotgun_snaps + no_huddle_snap +
                  (1+pass_pct|coach) + (1+plays|coach) + (1+yards_gained|coach) + (1+shotgun_snaps|coach) + (1+no_huddle_snap|coach),                                            data=team_sum2,
                  REML=FALSE)

epa.lmer2.full = lmer(epa_per_play ~ home_adv + pass_pct + plays + yards_gained + shotgun_snaps + no_huddle_snap +
                  (1+home_adv|coach) + (1+pass_pct|coach) + (1+plays|coach) + (1+yards_gained|coach) + (1+shotgun_snaps|coach) + (1+no_huddle_snap|coach),                       data=team_sum2,
                  REML=FALSE)

anova(epa.lmer2.full, epa.lmer2.null)

```


# Random Slopes vs Random Intercepts

## Random Intercept Model {.smaller}

```{r}
#| include: true
#| echo: true
#| warning: false
#| message: false
coef(epa.lmer1)
```
- Each coach is assigned a different intercept, but the fixed effects are the same for all coaches. 
- This model is called a Random Intercept model; we are accounting for baseline differences in epa per play. 

## Random Slope Model {.smaller}

```{r}
#| include: true
#| echo: true
#| warning: false
#| message: false

# LMM - random slopes
epa.lmer2 = lmer(epa_per_play ~ pass_pct + plays + yards_gained + shotgun_snaps + no_huddle_snap +
                (1+pass_pct|coach) + (1+plays|coach) + (1+yards_gained|coach) + (1+shotgun_snaps|coach) + (1+no_huddle_snap|coach), data=team_sum2)
```

- Alternatively, in a Random Slope model, each coach is allowed to have a different intercept, as well as different slopes for the effect of number of plays ran and percentage of pass plays, yards gained etc.
- Note the only difference is the notation for the random effects. 
- (1+pass_pct|coach) means we are telling the model to expect differing baseline levels of epa per play (the intercept represented by 1) as well as differing baseline levels of pass_pct  


## Coefficients {.smaller}

```{r}
coef(epa.lmer2)
```
- here the model is expecting different baseline levels of plays ran, pass percentage, yards gained, etc.
- despite the individual variation of pass_pct, all the values are negative and very close to each other. We see consistency with how often coaches throw the ball. The variation in number of shotgun snaps an offense runs is much wider.


## Conclusion {.smaller}

Using a mixed-effects approach we are able to find the additional effect a subject (a team's coach in our case) has on a offenses success, measured by epa per play. After accounting for the fixed effects plays ran, percentage of pass plays, yards gained, shotgun snaps, and no huddle snaps, our random effect coefficient for coaching showed an additional change 0.02 epa per play due to coaching and no change in epa per play due to home field advantage.
